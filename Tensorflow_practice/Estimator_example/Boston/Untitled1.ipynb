{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joon_Park\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc(object):\n",
    "    def __init__(self):\n",
    "        self.W = tf.get_variable('out',shape = [10,10] , dtype= tf.float32 )\n",
    "    def __call__ (self, inputs):\n",
    "        return tf.matmul(inputs, self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(inputs, scope, reuse):\n",
    "    with tf.variable_scope(scope, reuse = reuse):\n",
    "        _fc = fc()\n",
    "        return _fc(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 10])\n",
    "with tf.variable_scope('test'):\n",
    "    with tf.name_scope('FC'):\n",
    "        x1 = fc_layer(inputs, 'my', False)\n",
    "        x2 = fc_layer(x1, 'my2', False)\n",
    "        x3 = fc_layer(x2, 'siv', False)\n",
    "\n",
    "    with tf.name_scope('SS'):\n",
    "        x4 = fc_layer(x2,'sv',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'my2/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'siv/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'sv/out:0' shape=(10, 10) dtype=float32_ref>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'test/my/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'test/my2/out:0' shape=(10, 10) dtype=float32_ref>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection('variables', 'test/my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainable_variables', 'variables']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_all_collection_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'my2/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'siv/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'sv/out:0' shape=(10, 10) dtype=float32_ref>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_collection('trainable_variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_to_collections('nd',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__varscope',),\n",
       " ('__variable_store',),\n",
       " 'trainable_variables',\n",
       " 'variables',\n",
       " 'si',\n",
       " 'siv',\n",
       " 'nd']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainable_variables', 'variables', 'si', 'siv', 'nd']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_all_collection_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'my2/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'siv/out:0' shape=(10, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'sv/out:0' shape=(10, 10) dtype=float32_ref>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_collection('trainable_variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_collection_ref('my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "from utils import *\n",
    "from ops import *\n",
    "\n",
    "class NTMCell(object):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 mem_size=128, mem_dim=20, controller_dim=100,\n",
    "                 controller_layer_size=1, shift_range=1,\n",
    "                 write_head_size=1, read_head_size=1):\n",
    "        \"\"\"Initialize the parameters for an NTM cell.\n",
    "        Args:\n",
    "            input_dim: int, The number of units in the LSTM cell\n",
    "            output_dim: int, The dimensionality of the inputs into the LSTM cell\n",
    "            mem_size: (optional) int, The size of memory [128]\n",
    "            mem_dim: (optional) int, The dimensionality for memory [20]\n",
    "            controller_dim: (optional) int, The dimensionality for controller [100]\n",
    "            controller_layer_size: (optional) int, The size of controller layer [1]\n",
    "        \"\"\"\n",
    "        # initialize configs\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_dim = mem_dim\n",
    "        self.controller_dim = controller_dim\n",
    "        self.controller_layer_size = controller_layer_size\n",
    "        self.shift_range = shift_range\n",
    "        self.write_head_size = write_head_size\n",
    "        self.read_head_size = read_head_size\n",
    "\n",
    "        self.depth = 0\n",
    "        self.states = []\n",
    "\n",
    "    def __call__(self, input_, state=None, scope=None):\n",
    "        \"\"\"Run one step of NTM.\n",
    "        Args:\n",
    "            inputs: input Tensor, 2D, 1 x input_size.\n",
    "            state: state Dictionary which contains M, read_w, write_w, read,\n",
    "                output, hidden.\n",
    "            scope: VariableScope for the created subgraph; defaults to class name.\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - A 2D, batch x output_dim, Tensor representing the output of the LSTM\n",
    "                after reading \"input_\" when previous state was \"state\".\n",
    "                Here output_dim is:\n",
    "                     num_proj if num_proj was set,\n",
    "                     num_units otherwise.\n",
    "            - A 2D, batch x state_size, Tensor representing the new state of LSTM\n",
    "                after reading \"input_\" when previous state was \"state\".\n",
    "        \"\"\"\n",
    "        if state == None:\n",
    "            _, state = self.initial_state()\n",
    "\n",
    "        M_prev = state['M']\n",
    "        read_w_list_prev = state['read_w']\n",
    "        write_w_list_prev = state['write_w']\n",
    "        read_list_prev = state['read']\n",
    "        output_list_prev = state['output']\n",
    "        hidden_list_prev = state['hidden']\n",
    "\n",
    "        # build a controller\n",
    "        output_list, hidden_list = self.build_controller(input_, read_list_prev,\n",
    "                                                         output_list_prev,\n",
    "                                                         hidden_list_prev)\n",
    "\n",
    "        # last output layer from LSTM controller\n",
    "        last_output = output_list[-1]\n",
    "\n",
    "        # build a memory\n",
    "        M, read_w_list, write_w_list, read_list = self.build_memory(M_prev,\n",
    "                                                                    read_w_list_prev,\n",
    "                                                                    write_w_list_prev,\n",
    "                                                                    last_output)\n",
    "\n",
    "        # get a new output\n",
    "        new_output, new_output_logit = self.new_output(last_output)\n",
    "\n",
    "        state = {\n",
    "            'M': M,\n",
    "            'read_w': read_w_list,\n",
    "            'write_w': write_w_list,\n",
    "            'read': read_list,\n",
    "            'output': output_list,\n",
    "            'hidden': hidden_list,\n",
    "        }\n",
    "\n",
    "        self.depth += 1\n",
    "        self.states.append(state)\n",
    "\n",
    "        return new_output, new_output_logit, state\n",
    "\n",
    "    def new_output(self, output):\n",
    "        \"\"\"Logistic sigmoid output layers.\"\"\"\n",
    "\n",
    "        with tf.variable_scope('output'):\n",
    "            logit = Linear(output, self.output_dim, name='output')\n",
    "            return tf.sigmoid(logit), logit\n",
    "\n",
    "    def build_controller(self, input_,\n",
    "                         read_list_prev, output_list_prev, hidden_list_prev):\n",
    "        \"\"\"Build LSTM controller.\"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"controller\"):\n",
    "            output_list = []\n",
    "            hidden_list = []\n",
    "            for layer_idx in xrange(self.controller_layer_size):\n",
    "                o_prev = output_list_prev[layer_idx]\n",
    "                h_prev = hidden_list_prev[layer_idx]\n",
    "\n",
    "                if layer_idx == 0:\n",
    "                    def new_gate(gate_name):\n",
    "                        return linear([input_, o_prev] + read_list_prev,\n",
    "                                      output_size = self.controller_dim,\n",
    "                                      bias = True,\n",
    "                                      scope = \"%s_gate_%s\" % (gate_name, layer_idx))\n",
    "                else:\n",
    "                    def new_gate(gate_name):\n",
    "                        return linear([output_list[-1], o_prev],\n",
    "                                      output_size = self.controller_dim,\n",
    "                                      bias = True,\n",
    "                                      scope=\"%s_gate_%s\" % (gate_name, layer_idx))\n",
    "\n",
    "                # input, forget, and output gates for LSTM\n",
    "                i = tf.sigmoid(new_gate('input'))\n",
    "                f = tf.sigmoid(new_gate('forget'))\n",
    "                o = tf.sigmoid(new_gate('output'))\n",
    "                update = tf.tanh(new_gate('update'))\n",
    "\n",
    "                # update the sate of the LSTM cell\n",
    "                hid = tf.add_n([f * h_prev, i * update])\n",
    "                out = o * tf.tanh(hid)\n",
    "\n",
    "                hidden_list.append(hid)\n",
    "                output_list.append(out)\n",
    "\n",
    "            return output_list, hidden_list\n",
    "\n",
    "    def build_memory(self, M_prev, read_w_list_prev, write_w_list_prev, last_output):\n",
    "        \"\"\"Build a memory to read & write.\"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"memory\"):\n",
    "            # 3.1 Reading\n",
    "            if self.read_head_size == 1:\n",
    "                read_w_prev = read_w_list_prev[0]\n",
    "\n",
    "                read_w, read = self.build_read_head(M_prev, tf.squeeze(read_w_prev),\n",
    "                                                    last_output, 0)\n",
    "                read_w_list = [read_w]\n",
    "                read_list = [read]\n",
    "            else:\n",
    "                read_w_list = []\n",
    "                read_list = []\n",
    "\n",
    "                for idx in xrange(self.read_head_size):\n",
    "                    read_w_prev_idx = read_w_list_prev[idx]\n",
    "                    read_w_idx, read_idx = self.build_read_head(M_prev, read_w_prev_idx,\n",
    "                                                                last_output, idx)\n",
    "\n",
    "                    read_w_list.append(read_w_idx)\n",
    "                    read_list.append(read_idx)\n",
    "\n",
    "            # 3.2 Writing\n",
    "            if self.write_head_size == 1:\n",
    "                write_w_prev = write_w_list_prev[0]\n",
    "\n",
    "                write_w, write, erase = self.build_write_head(M_prev,\n",
    "                                                              tf.squeeze(write_w_prev),\n",
    "                                                              last_output, 0)\n",
    "\n",
    "                M_erase = tf.ones([self.mem_size, self.mem_dim]) \\\n",
    "                                  - outer_product(write_w, erase)\n",
    "                M_write = outer_product(write_w, write)\n",
    "\n",
    "                write_w_list = [write_w]\n",
    "            else:\n",
    "                write_w_list = []\n",
    "                write_list = []\n",
    "                erase_list = []\n",
    "\n",
    "                M_erases = []\n",
    "                M_writes = []\n",
    "\n",
    "                for idx in xrange(self.write_head_size):\n",
    "                    write_w_prev_idx = write_w_list_prev[idx]\n",
    "\n",
    "                    write_w_idx, write_idx, erase_idx = \\\n",
    "                        self.build_write_head(M_prev, write_w_prev_idx,\n",
    "                                              last_output, idx)\n",
    "\n",
    "                    write_w_list.append(tf.transpose(write_w_idx))\n",
    "                    write_list.append(write_idx)\n",
    "                    erase_list.append(erase_idx)\n",
    "\n",
    "                    M_erases.append(tf.ones([self.mem_size, self.mem_dim]) \\\n",
    "                                    - outer_product(write_w_idx, erase_idx))\n",
    "                    M_writes.append(outer_product(write_w_idx, write_idx))\n",
    "\n",
    "                M_erase = reduce(lambda x, y: x*y, M_erases)\n",
    "                M_write = tf.add_n(M_writes)\n",
    "\n",
    "            M = M_prev * M_erase + M_write\n",
    "\n",
    "            return M, read_w_list, write_w_list, read_list\n",
    "\n",
    "    def build_read_head(self, M_prev, read_w_prev, last_output, idx):\n",
    "        return self.build_head(M_prev, read_w_prev, last_output, True, idx)\n",
    "\n",
    "    def build_write_head(self, M_prev, write_w_prev, last_output, idx):\n",
    "        return self.build_head(M_prev, write_w_prev, last_output, False, idx)\n",
    "\n",
    "    def build_head(self, M_prev, w_prev, last_output, is_read, idx):\n",
    "        scope = \"read\" if is_read else \"write\"\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            # Figure 2.\n",
    "            # Amplify or attenuate the precision\n",
    "            with tf.variable_scope(\"k\"):\n",
    "                k = tf.tanh(Linear(last_output, self.mem_dim, name='k_%s' % idx))\n",
    "            # Interpolation gate\n",
    "            with tf.variable_scope(\"g\"):\n",
    "                g = tf.sigmoid(Linear(last_output, 1, name='g_%s' % idx))\n",
    "            # shift weighting\n",
    "            with tf.variable_scope(\"s_w\"):\n",
    "                w = Linear(last_output, 2 * self.shift_range + 1, name='s_w_%s' % idx)\n",
    "                s_w = softmax(w)\n",
    "            with tf.variable_scope(\"beta\"):\n",
    "                beta  = tf.nn.softplus(Linear(last_output, 1, name='beta_%s' % idx))\n",
    "            with tf.variable_scope(\"gamma\"):\n",
    "                gamma = tf.add(tf.nn.softplus(Linear(last_output, 1, name='gamma_%s' % idx)),\n",
    "                               tf.constant(1.0))\n",
    "\n",
    "            # 3.3.1\n",
    "            # Cosine similarity\n",
    "            similarity = smooth_cosine_similarity(M_prev, k) # [mem_size x 1]\n",
    "            # Focusing by content\n",
    "            content_focused_w = softmax(scalar_mul(similarity, beta))\n",
    "\n",
    "            # 3.3.2\n",
    "            # Focusing by location\n",
    "            gated_w = tf.add_n([\n",
    "                scalar_mul(content_focused_w, g),\n",
    "                scalar_mul(w_prev, (tf.constant(1.0) - g))\n",
    "            ])\n",
    "\n",
    "            # Convolutional shifts\n",
    "            conv_w = circular_convolution(gated_w, s_w)\n",
    "\n",
    "            # Sharpening\n",
    "            powed_conv_w = tf.pow(conv_w, gamma)\n",
    "            w = powed_conv_w / tf.reduce_sum(powed_conv_w)\n",
    "\n",
    "            if is_read:\n",
    "                # 3.1 Reading\n",
    "                read = matmul(tf.transpose(M_prev), w)\n",
    "                return w, read\n",
    "            else:\n",
    "                # 3.2 Writing\n",
    "                erase = tf.sigmoid(Linear(last_output, self.mem_dim, name='erase_%s' % idx))\n",
    "                add = tf.tanh(Linear(last_output, self.mem_dim, name='add_%s' % idx))\n",
    "                return w, add, erase\n",
    "\n",
    "    def initial_state(self, dummy_value=0.0):\n",
    "        self.depth = 0\n",
    "        self.states = []\n",
    "        with tf.variable_scope(\"init_cell\"):\n",
    "            # always zero\n",
    "            dummy = tf.Variable(tf.constant([[dummy_value]], dtype=tf.float32))\n",
    "\n",
    "            # memory\n",
    "            M_init_linear = tf.tanh(Linear(dummy, self.mem_size * self.mem_dim,\n",
    "                                    name='M_init_linear'))\n",
    "            M_init = tf.reshape(M_init_linear, [self.mem_size, self.mem_dim])\n",
    "\n",
    "            # read weights\n",
    "            read_w_list_init = []\n",
    "            read_list_init = []\n",
    "            for idx in xrange(self.read_head_size):\n",
    "                read_w_idx = Linear(dummy, self.mem_size, is_range=True, \n",
    "                                    squeeze=True, name='read_w_%d' % idx)\n",
    "                read_w_list_init.append(softmax(read_w_idx))\n",
    "\n",
    "                read_init_idx = Linear(dummy, self.mem_dim,\n",
    "                                       squeeze=True, name='read_init_%d' % idx)\n",
    "                read_list_init.append(tf.tanh(read_init_idx))\n",
    "\n",
    "            # write weights\n",
    "            write_w_list_init = []\n",
    "            for idx in xrange(self.write_head_size):\n",
    "                write_w_idx = Linear(dummy, self.mem_size, is_range=True,\n",
    "                                     squeeze=True, name='write_w_%s' % idx)\n",
    "                write_w_list_init.append(softmax(write_w_idx))\n",
    "\n",
    "            # controller state\n",
    "            output_init_list = []                     \n",
    "            hidden_init_list = []                     \n",
    "            for idx in xrange(self.controller_layer_size):\n",
    "                output_init_idx = Linear(dummy, self.controller_dim,\n",
    "                                         squeeze=True, name='output_init_%s' % idx)\n",
    "                output_init_list.append(tf.tanh(output_init_idx))\n",
    "                hidden_init_idx = Linear(dummy, self.controller_dim,\n",
    "                                         squeeze=True, name='hidden_init_%s' % idx)\n",
    "                hidden_init_list.append(tf.tanh(hidden_init_idx))\n",
    "\n",
    "            output = tf.tanh(Linear(dummy, self.output_dim, name='new_output'))\n",
    "\n",
    "            state = {\n",
    "                'M': M_init,\n",
    "                'read_w': read_w_list_init,\n",
    "                'write_w': write_w_list_init,\n",
    "                'read': read_list_init,\n",
    "                'output': output_init_list,\n",
    "                'hidden': hidden_init_list\n",
    "            }\n",
    "\n",
    "            self.depth += 1\n",
    "            self.states.append(state)\n",
    "\n",
    "            return output, state\n",
    "\n",
    "    def get_memory(self, depth=None):\n",
    "        depth = depth if depth else self.depth\n",
    "        return self.states[depth - 1]['M']\n",
    "\n",
    "    def get_read_weights(self, depth=None):\n",
    "        depth = depth if depth else self.depth\n",
    "        return self.states[depth - 1]['read_w']\n",
    "\n",
    "    def get_write_weights(self, depth=None):\n",
    "        depth = depth if depth else self.depth\n",
    "        return self.states[depth - 1]['write_w']\n",
    "\n",
    "    def get_read_vector(self, depth=None):\n",
    "        depth = depth if depth else self.depth\n",
    "        return self.states[depth - 1]['read']\n",
    "\n",
    "    def print_read_max(self, sess):\n",
    "        read_w_list = sess.run(self.get_read_weights())\n",
    "\n",
    "        fmt = \"%-4d %.4f\"\n",
    "        if self.read_head_size == 1:\n",
    "            print(fmt % (argmax(read_w_list[0])))\n",
    "        else:\n",
    "            for idx in xrange(self.read_head_size):\n",
    "                print(fmt % np.argmax(read_w_list[idx]))\n",
    "\n",
    "    def print_write_max(self, sess):\n",
    "        write_w_list = sess.run(self.get_write_weights())\n",
    "\n",
    "        fmt = \"%-4d %.4f\"\n",
    "        if self.write_head_size == 1:\n",
    "            print(fmt % (argmax(write_w_list[0])))\n",
    "        else:\n",
    "            for idx in xrange(self.write_head_size):\n",
    "                print(fmt % argmax(write_w_list[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

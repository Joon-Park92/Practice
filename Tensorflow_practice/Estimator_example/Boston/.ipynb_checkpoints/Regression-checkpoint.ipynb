{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Regression using the DNNRegressor Estimator.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import imports85  # pylint: disable=g-bad-import-order\n",
    "\n",
    "STEPS = 1000\n",
    "PRICE_NORM_FACTOR = 1000\n",
    "\n",
    "\n",
    "def my_dnn_regression_fn(features, labels, mode, params):\n",
    "  \"\"\"A model function implementing DNN regression for a custom Estimator.\"\"\"\n",
    "\n",
    "  # Extract the input into a dense layer, according to the feature_columns.\n",
    "  top = tf.feature_column.input_layer(features, params[\"feature_columns\"])\n",
    "\n",
    "  # Iterate over the \"hidden_units\" list of layer sizes, default is [20].\n",
    "  for units in params.get(\"hidden_units\", [20]):\n",
    "    # Add a hidden layer, densely connected on top of the previous layer.\n",
    "    top = tf.layers.dense(inputs=top, units=units, activation=tf.nn.relu)\n",
    "\n",
    "  # Connect a linear output layer on top.\n",
    "  output_layer = tf.layers.dense(inputs=top, units=1)\n",
    "\n",
    "  # Reshape the output layer to a 1-dim Tensor to return predictions\n",
    "  predictions = tf.squeeze(output_layer, 1)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # In `PREDICT` mode we only need to return predictions.\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, predictions={\"price\": predictions})\n",
    "\n",
    "  # Calculate loss using mean squared error\n",
    "  average_loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "\n",
    "  # Pre-made estimators use the total_loss instead of the average,\n",
    "  # so report total_loss for compatibility.\n",
    "  batch_size = tf.shape(labels)[0]\n",
    "  total_loss = tf.to_float(batch_size) * average_loss\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer)\n",
    "    optimizer = optimizer(params.get(\"learning_rate\", None))\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=average_loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=total_loss, train_op=train_op)\n",
    "\n",
    "  # In evaluation mode we will calculate evaluation metrics.\n",
    "  assert mode == tf.estimator.ModeKeys.EVAL\n",
    "\n",
    "  # Calculate root mean squared error\n",
    "  rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "  # Add the rmse to the collection of evaluation metrics.\n",
    "  eval_metrics = {\"rmse\": rmse}\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      # Report sum of error for compatibility with pre-made estimators\n",
    "      loss=total_loss,\n",
    "      eval_metric_ops=eval_metrics)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "  \"\"\"Builds, trains, and evaluates the model.\"\"\"\n",
    "  assert len(argv) == 1\n",
    "  (train, test) = imports85.dataset()\n",
    "\n",
    "  # Switch the labels to units of thousands for better convergence.\n",
    "  def normalize_price(features, labels):\n",
    "    return features, labels / PRICE_NORM_FACTOR\n",
    "\n",
    "  train = train.map(normalize_price)\n",
    "  test = test.map(normalize_price)\n",
    "\n",
    "  # Build the training input_fn.\n",
    "  def input_train():\n",
    "    return (\n",
    "        # Shuffling with a buffer larger than the data set ensures\n",
    "        # that the examples are well mixed.\n",
    "        train.shuffle(1000).batch(128)\n",
    "        # Repeat forever\n",
    "        .repeat().make_one_shot_iterator().get_next())\n",
    "\n",
    "  # Build the validation input_fn.\n",
    "  def input_test():\n",
    "    return (test.shuffle(1000).batch(128)\n",
    "            .make_one_shot_iterator().get_next())\n",
    "\n",
    "  # The first way assigns a unique weight to each category. To do this you must\n",
    "  # specify the category's vocabulary (values outside this specification will\n",
    "  # receive a weight of zero). Here we specify the vocabulary using a list of\n",
    "  # options. The vocabulary can also be specified with a vocabulary file (using\n",
    "  # `categorical_column_with_vocabulary_file`). For features covering a\n",
    "  # range of positive integers use `categorical_column_with_identity`.\n",
    "  body_style_vocab = [\"hardtop\", \"wagon\", \"sedan\", \"hatchback\", \"convertible\"]\n",
    "  body_style = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key=\"body-style\", vocabulary_list=body_style_vocab)\n",
    "  make = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      key=\"make\", hash_bucket_size=50)\n",
    "\n",
    "  feature_columns = [\n",
    "      tf.feature_column.numeric_column(key=\"curb-weight\"),\n",
    "      tf.feature_column.numeric_column(key=\"highway-mpg\"),\n",
    "      # Since this is a DNN model, convert categorical columns from sparse\n",
    "      # to dense.\n",
    "      # Wrap them in an `indicator_column` to create a\n",
    "      # one-hot vector from the input.\n",
    "      tf.feature_column.indicator_column(body_style),\n",
    "      # Or use an `embedding_column` to create a trainable vector for each\n",
    "      # index.\n",
    "      tf.feature_column.embedding_column(make, dimension=3),\n",
    "  ]\n",
    "\n",
    "  # Build a custom Estimator, using the model_fn.\n",
    "  # `params` is passed through to the `model_fn`.\n",
    "  model = tf.estimator.Estimator(\n",
    "      model_fn=my_dnn_regression_fn,\n",
    "      params={\n",
    "          \"feature_columns\": feature_columns,\n",
    "          \"learning_rate\": 0.001,\n",
    "          \"optimizer\": tf.train.AdamOptimizer,\n",
    "          \"hidden_units\": [20, 20]\n",
    "      })\n",
    "\n",
    "  # Train the model.\n",
    "  model.train(input_fn=input_train, steps=STEPS)\n",
    "\n",
    "  # Evaluate how the model performs on data it has not yet seen.\n",
    "  eval_result = model.evaluate(input_fn=input_test)\n",
    "\n",
    "  # Print the Root Mean Square Error (RMSE).\n",
    "  print(\"\\n\" + 80 * \"*\")\n",
    "  print(\"\\nRMS error for the test set: ${:.0f}\"\n",
    "        .format(PRICE_NORM_FACTOR * eval_result[\"rmse\"]))\n",
    "\n",
    "  print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # The Estimator periodically generates \"INFO\" logs; make these logs visible.\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  tf.app.run(main=main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
